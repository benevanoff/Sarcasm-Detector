{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data\n",
        "For this project I will be using the dataset collected and annotated for Dr. Helen Gent's doctoral dissertation at the University of Illinois, which I was involved in preparing.\n",
        "\n",
        "The dataset comes from Jarvis Johnson's SadBoyz podcast and has been segmented to individual utterances and each segment has been labeled either \"ironic\" or \"non-ironic\""
      ],
      "metadata": {
        "id": "akUEHUcjaJcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/helengent/Irony-Recognition.git # clone Dr Gent's repo for the data"
      ],
      "metadata": {
        "id": "TFBAA2i0ePrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Transformer-Encoder Classifier"
      ],
      "metadata": {
        "id": "jMMn1IZ6dvLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install librosa\n",
        "\n",
        "import os, re\n",
        "import math, random\n",
        "import librosa\n",
        "import torch\n",
        "\n",
        "class PrunedCorpus:\n",
        "\n",
        "  def __init__(self, path):\n",
        "    self.len = 0\n",
        "    self.path = path\n",
        "    self.data = {} # key is episode num, val is list of (utterance file, utterance num, speaker, is_sarcasm) tuple\n",
        "    self.load_data()\n",
        "\n",
        "  def load_data(self):\n",
        "    for filename in os.listdir(self.path):\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "      self.len += 1\n",
        "      filename_split = filename.split('_')\n",
        "      episode_label = filename_split[0]\n",
        "      if episode_label not in self.data:\n",
        "        self.data[episode_label] = []\n",
        "      utterance_num = re.findall(r'\\d+', filename_split[1])[0]\n",
        "      speaker = filename_split[1][0]\n",
        "      is_sarcasm = \"-I.wav\" in filename\n",
        "      #print(filename, utterance_num, speaker, is_sarcasm)\n",
        "      self.data[episode_label].append((filename, utterance_num, speaker, is_sarcasm))\n",
        "    # sort the utterances once all files have been read\n",
        "    for episode, utterances in self.data.items():\n",
        "      episode_sorted = sorted(utterances, key=lambda x: int(x[1]))\n",
        "      self.data[episode] = episode_sorted\n",
        "\n",
        "  def get_episodes(self):\n",
        "    return list(self.data.keys())\n",
        "\n",
        "  def get_utterances(self, episode):\n",
        "    return self.data[episode]\n",
        "\n",
        "  def utterance_to_tensor(self, utterance:tuple):\n",
        "    audio, sr = librosa.load(self.path + \"/\" + utterance[0], sr=16000)\n",
        "    mel_spectrogram = librosa.feature.melspectrogram(y=audio).T # transpose so that the rows are time frames\n",
        "    mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
        "    normalized_mel_spectogram = librosa.util.normalize(mel_spectrogram) # normalize the audio to [-1, 1]\n",
        "    return torch.tensor(normalized_mel_spectogram).to('cuda')\n",
        "\n",
        "  def __iter__(self):\n",
        "    for episode in self.get_episodes():\n",
        "      for utterance in self.get_utterances(episode):\n",
        "        yield self.utterance_to_tensor(utterance)\n",
        "\n",
        "  def bool_to_tensor(self, arg):\n",
        "    if arg:\n",
        "      return torch.tensor([1.], dtype=float)\n",
        "    else:\n",
        "      return torch.tensor([0.], dtype=float)\n",
        "\n",
        "  def easy_trainset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    return {\n",
        "        \"tensors\" :[self.utterance_to_tensor(utt) for utt in episode],\n",
        "        \"labels\" : [self.bool_to_tensor(utt[3]) for utt in episode]\n",
        "    }\n",
        "\n",
        "  def trainset(self):\n",
        "    # use all episodes except SBep13 SBep19 and only use d and c speakers\n",
        "    utts = []\n",
        "    for name, episode in self.data.items():\n",
        "      if name == \"SBep19\" or name == \"SBep13\":\n",
        "        continue\n",
        "      for utterance in episode:\n",
        "        if utterance[2] in ['c', 'd']:\n",
        "          utts.append(utterance)\n",
        "    # shuffle and rebalance ironic and non ironic utts\n",
        "    random.shuffle(utts)\n",
        "    ironic = [utt for utt in utts if utt[3]]\n",
        "    non_ironic = [utt for utt in utts if not utt[3]]\n",
        "    max_len = min(len(ironic), len(non_ironic))\n",
        "    utts_balanced = ironic[0:max_len] + non_ironic[0:max_len]\n",
        "    random.shuffle(utts_balanced)\n",
        "\n",
        "    tensors = []\n",
        "    labels = []\n",
        "    for utt in utts_balanced:\n",
        "      tensors.append(self.utterance_to_tensor(utterance))\n",
        "      labels.append(utterance[3])\n",
        "    return {\n",
        "        \"tensors\": tensors,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "  def testset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    return {\n",
        "        \"tensors\" :[self.utterance_to_tensor(utt) for utt in episode],\n",
        "        \"labels\" : [self.bool_to_tensor(utt[3]) for utt in episode]\n",
        "    }\n",
        "\n",
        "  def devset(self):\n",
        "    episode = self.data[\"SBep19\"]\n",
        "    return {\n",
        "        \"tensors\" :[self.utterance_to_tensor(utt) for utt in episode],\n",
        "        \"labels\" : [self.bool_to_tensor(utt[3]) for utt in episode]\n",
        "    }\n",
        "\n",
        "corpus = PrunedCorpus('Irony-Recognition/AudioData/GatedPruned3')\n",
        "\n",
        "# look at distribution of speakers over episodes\n",
        "for episode in corpus.get_episodes():\n",
        "  speakers = set()\n",
        "  for utterance in corpus.get_utterances(episode):\n",
        "    speakers.add(utterance[2])\n",
        "  print(episode, speakers)\n",
        "\n",
        "class SarcasmClassifier(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, dim):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.dim = dim\n",
        "    self.transformer_encoder_layer = torch.nn.TransformerEncoderLayer(d_model=dim, nhead=32)\n",
        "    self.transformer_encoder = torch.nn.TransformerEncoder(self.transformer_encoder_layer, num_layers=12)\n",
        "    self.attention_linear = torch.nn.Linear(dim, 1)\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 1),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    def positional_encoding(pos, d_model):\n",
        "      pos_enc = torch.zeros(d_model).to('cuda')\n",
        "      for i in range(0, d_model, 2):\n",
        "        pos_enc[i] = math.sin(pos / (10000 ** (2 * i / d_model)))\n",
        "        pos_enc[i + 1] = math.cos(pos / (10000 ** (2 * i / d_model)))\n",
        "      return pos_enc\n",
        "\n",
        "    positional_encodings = torch.zeros(len(x), self.dim).to('cuda')\n",
        "    for pos in range(len(x)):\n",
        "      positional_encodings[pos, :] = positional_encoding(pos, self.dim)\n",
        "    encoded_output = self.transformer_encoder(positional_encodings)\n",
        "    attention_scores = self.attention_linear(encoded_output)\n",
        "    attention_weights = torch.nn.functional.softmax(attention_scores, dim=0)\n",
        "    x_weighted = (attention_weights * encoded_output).sum(dim=0)\n",
        "    return self.mlp(x_weighted)\n",
        "\n",
        "dim = 128\n",
        "model = SarcasmClassifier(dim).to('cuda')\n",
        "\n",
        "# train\n",
        "trainset = corpus.trainset()\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRAIN_EPOCHS = 10\n",
        "corpus_size = len(trainset[\"tensors\"])\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    point_counter = 0\n",
        "    assert len(trainset[\"tensors\"]) == len(trainset[\"labels\"])\n",
        "    for i in range(len(trainset[\"tensors\"])):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(trainset[\"tensors\"][i])\n",
        "        target_tensor = torch.tensor([1.], dtype=torch.float).to('cuda') if trainset[\"labels\"][i] else torch.tensor([0.], dtype=torch.float).to('cuda')\n",
        "        #print(output, target_tensor)\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        point_counter += 1\n",
        "        if point_counter % 25 == 0:\n",
        "          print(f\"Loss: {loss}, {point_counter}/{corpus_size}\")\n",
        "    # evaluate against devset\n",
        "    with torch.no_grad():\n",
        "      evalset = corpus.devset()\n",
        "      correct = 0\n",
        "      for i in range(len(evalset[\"tensors\"])):\n",
        "        output = model(evalset[\"tensors\"][i])\n",
        "        pred = True if torch.sigmoid(output) >= 0.5 else False\n",
        "        if pred == evalset[\"labels\"][i]:\n",
        "          correct += 1\n",
        "        print(output, torch.sigmoid(output), evalset[\"labels\"][i])\n",
        "# evaluate against testset\n",
        "with torch.no_grad():\n",
        "  evalset = corpus.testset()\n",
        "  correct = 0\n",
        "  for i in range(len(evalset[\"tensors\"])):\n",
        "    output = model(evalset[\"tensors\"][i])\n",
        "    pred = True if torch.sigmoid(output) >= 0.5 else False\n",
        "    if pred == evalset[\"labels\"][i]:\n",
        "      correct += 1\n",
        "    print(output, torch.sigmoid(output), evalset[\"labels\"][i])\n"
      ],
      "metadata": {
        "id": "WkoKcHGMeC2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GshseA7zhLz"
      },
      "source": [
        "# Fine-tuned Wav2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of learning relationships directly from spectograms from scratch, we can use a pre-trained model to try to make up for our small (~10hrs) dataset. First, we try fine-tuning Wav2Vec to produce vector embeddings for the audio clip. To convert the time series embeddings to a single vector to be classified, I will use a sinlge layer forward LSTM, as Dr. Gent did with the features for her classifier."
      ],
      "metadata": {
        "id": "816BlB-l2TGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcqkPDyHbP7g"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "import os, re, random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer\n",
        "import soundfile as sf\n",
        "\n",
        "class PrunedCorpus:\n",
        "\n",
        "  def __init__(self, path):\n",
        "    self.len = 0\n",
        "    self.path = path\n",
        "    self.data = {} # key is episode num, val is list of (utterance file, utterance num, speaker, is_sarcasm) tuple\n",
        "    self.load_data()\n",
        "    self.tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "    self.model_wav2vec = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "  def load_data(self):\n",
        "    for filename in os.listdir(self.path):\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "      self.len += 1\n",
        "      filename_split = filename.split('_')\n",
        "      episode_label = filename_split[0]\n",
        "      if episode_label not in self.data:\n",
        "        self.data[episode_label] = []\n",
        "      utterance_num = re.findall(r'\\d+', filename_split[1])[0]\n",
        "      speaker = filename_split[1][0]\n",
        "      is_sarcasm = \"-I.wav\" in filename\n",
        "      #print(filename, utterance_num, speaker, is_sarcasm)\n",
        "      self.data[episode_label].append((filename, utterance_num, speaker, is_sarcasm))\n",
        "    # sort the utterances once all files have been read\n",
        "    for episode, utterances in self.data.items():\n",
        "      episode_sorted = sorted(utterances, key=lambda x: int(x[1]))\n",
        "      self.data[episode] = episode_sorted\n",
        "\n",
        "  def get_episodes(self):\n",
        "    return list(self.data.keys())\n",
        "\n",
        "  def get_utterances(self, episode):\n",
        "    return self.data[episode]\n",
        "\n",
        "  def utterance_to_tensor(self, utterance:tuple, with_grad=False):\n",
        "    audio_input, _ = sf.read(self.path + \"/\" + utterance[0])\n",
        "    input_values = self.tokenizer(audio_input, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "    if with_grad:\n",
        "      h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    return h.squeeze(0)\n",
        "\n",
        "  def __iter__(self):\n",
        "    for episode in self.get_episodes():\n",
        "      for utterance in self.get_utterances(episode):\n",
        "        yield self.utterance_to_tensor(utterance)\n",
        "\n",
        "  def bool_to_tensor(self, arg):\n",
        "    if arg:\n",
        "      return torch.tensor([1.], dtype=torch.float)\n",
        "    else:\n",
        "      return torch.tensor([0.], dtype=torch.float)\n",
        "\n",
        "  def easy_trainset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    utts = [utt for utt in episode]\n",
        "    # now balance\n",
        "    ironic_utts = [utt for utt in episode if utt[3]]\n",
        "    non_ironic_utts_all = [utt for utt in episode if not utt[3]]\n",
        "    non_ironic_balanced = []\n",
        "    i = 0\n",
        "    for x in non_ironic_utts_all:\n",
        "      if i % 6 == 0:\n",
        "        non_ironic_balanced.append(x)\n",
        "      i += 1\n",
        "    balanced_utts = ironic_utts + non_ironic_balanced\n",
        "    random.shuffle(balanced_utts)\n",
        "    print(balanced_utts)\n",
        "    return balanced_utts\n",
        "\n",
        "  def trainset(self):\n",
        "    # use all episodes except SBep13 and SBep19 (devset and testset) and only use d and c speakers\n",
        "    utts = []\n",
        "    for name, episode in self.data.items():\n",
        "      if name == \"SBep19\" or name == \"SBep13\":\n",
        "        continue # dont include testset episode in trainset\n",
        "      for utterance in episode:\n",
        "        if utterance[2] in ['c', 'd']:\n",
        "          utts.append(utterance)\n",
        "    # shuffle and rebalance ironic and non ironic utts\n",
        "    random.shuffle(utts)\n",
        "    ironic = [utt for utt in utts if utt[3]]\n",
        "    non_ironic = [utt for utt in utts if not utt[3]]\n",
        "    max_len = min(len(ironic), len(non_ironic))\n",
        "    utts_balanced = ironic[0:max_len] + non_ironic[0:max_len]\n",
        "    random.shuffle(utts_balanced)\n",
        "    return utts_balanced\n",
        "\n",
        "  def testset(self):\n",
        "    episode = self.data[\"SBep19\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "corpus = PrunedCorpus('Irony-Recognition/AudioData/GatedPruned3')\n",
        "\n",
        "# look at distribution of speakers over episodes\n",
        "for episode in corpus.get_episodes():\n",
        "  speakers = set()\n",
        "  for utterance in corpus.get_utterances(episode):\n",
        "    speakers.add(utterance[2])\n",
        "  print(episode, speakers)\n",
        "\n",
        "# look at distribution of tags over episodes\n",
        "sarcastic_utts = []\n",
        "non_sarcastic_utts = []\n",
        "\n",
        "class SarcasmClassifier(torch.nn.Module):\n",
        "  # with LSTM pooling\n",
        "  def __init__(self, dim):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(dim, 512, 1, batch_first=True, bidirectional=False)\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    h, (hn, _) = self.lstm(x)\n",
        "    hn = hn.squeeze(0)\n",
        "    print(hn.shape)\n",
        "    return self.mlp(hn)\n",
        "\n",
        "dim = 768\n",
        "model = SarcasmClassifier(dim)\n",
        "\n",
        "# train\n",
        "trainset = corpus.trainset()\n",
        "# first look at balance of labels\n",
        "trainset_size = len(trainset)\n",
        "trainset_ironic_size = len([x for x in trainset if x[3]])\n",
        "trainset_non_ironic_size = trainset_size - trainset_ironic_size\n",
        "print(\"trainset data:\", trainset_size, trainset_ironic_size, trainset_non_ironic_size)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "TRAIN_EPOCHS = 1\n",
        "BATCH_SIZE = 16\n",
        "point_counter = 0\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    remaining_examples = trainset_size\n",
        "    i = 0\n",
        "    while remaining_examples >= BATCH_SIZE:\n",
        "      optimizer.zero_grad()\n",
        "      # get batch\n",
        "      batch_examples = trainset[i:i+BATCH_SIZE]\n",
        "      batch_labels = torch.stack([corpus.bool_to_tensor(x[3]) for x in batch_examples])\n",
        "      batch_examples = [corpus.utterance_to_tensor(x) for x in batch_examples]\n",
        "      lengths = [seq.size(0) for seq in batch_examples]\n",
        "      padded_sequences = pad_sequence(batch_examples, batch_first=True)\n",
        "      packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
        "      i += BATCH_SIZE\n",
        "      remaining_examples -= BATCH_SIZE\n",
        "      # now calc loss and backprop\n",
        "      y_hat = model(packed_sequences)\n",
        "      print(y_hat, batch_labels)\n",
        "      loss = criterion(y_hat, batch_labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      point_counter += 1\n",
        "      if point_counter % 5 == 0:\n",
        "          print(f\"Loss: {loss}, {point_counter*BATCH_SIZE}/{trainset_size}, {y_hat}, {batch_labels}\")\n",
        "\n",
        "    # evaluate\n",
        "    print(\"evaluating\")\n",
        "    with torch.no_grad():\n",
        "      evalset = corpus.testset()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for example in evalset:\n",
        "        x = corpus.utterance_to_tensor(example, with_grad=False)\n",
        "        y = corpus.bool_to_tensor(example[3])\n",
        "        y_hat = torch.sigmoid(model(x))\n",
        "        y_pred = torch.round(y_hat)\n",
        "        print(x.shape, y, y_pred, y_hat)\n",
        "        if (y_hat[0] >= 0.5 and y[0] >= 0.5) or (y_hat[0] < 0.5 and y[0] < 0.5):\n",
        "          correct += 1\n",
        "        total += 1\n",
        "      print(f'{correct}/{total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I try swapping the original Wav2Vec model with Wav2Vec-Conformer, which uses convolutions (helpful in vision/audio!!) instead of linear layers in the transformer blocks"
      ],
      "metadata": {
        "id": "feqJRsWbw0Yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "import os, re, random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from transformers import Wav2Vec2ConformerModel, AutoProcessor\n",
        "import soundfile as sf\n",
        "\n",
        "class PrunedCorpus:\n",
        "\n",
        "  def __init__(self, path):\n",
        "    self.len = 0\n",
        "    self.path = path\n",
        "    self.data = {} # key is episode num, val is list of (utterance file, utterance num, speaker, is_sarcasm) tuple\n",
        "    self.load_data()\n",
        "    self.tokenizer = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
        "    self.model_wav2vec = Wav2Vec2ConformerModel.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\").to('cuda')\n",
        "\n",
        "  def load_data(self):\n",
        "    for filename in os.listdir(self.path):\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "      self.len += 1\n",
        "      filename_split = filename.split('_')\n",
        "      episode_label = filename_split[0]\n",
        "      if episode_label not in self.data:\n",
        "        self.data[episode_label] = []\n",
        "      utterance_num = re.findall(r'\\d+', filename_split[1])[0]\n",
        "      speaker = filename_split[1][0]\n",
        "      is_sarcasm = \"-I.wav\" in filename\n",
        "      #print(filename, utterance_num, speaker, is_sarcasm)\n",
        "      self.data[episode_label].append((filename, utterance_num, speaker, is_sarcasm))\n",
        "    # sort the utterances once all files have been read\n",
        "    for episode, utterances in self.data.items():\n",
        "      episode_sorted = sorted(utterances, key=lambda x: int(x[1]))\n",
        "      self.data[episode] = episode_sorted\n",
        "\n",
        "  def get_episodes(self):\n",
        "    return list(self.data.keys())\n",
        "\n",
        "  def get_utterances(self, episode):\n",
        "    return self.data[episode]\n",
        "\n",
        "  def utterance_to_tensor(self, utterance:tuple, with_grad=False):\n",
        "    audio_input, _ = sf.read(self.path + \"/\" + utterance[0])\n",
        "    input_values = self.tokenizer(audio_input, sampling_rate=16000, return_tensors=\"pt\", padding=\"longest\").input_values\n",
        "    input_values = input_values.to('cuda')\n",
        "    if with_grad:\n",
        "      h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    return h.squeeze(0)\n",
        "\n",
        "  def __iter__(self):\n",
        "    for episode in self.get_episodes():\n",
        "      for utterance in self.get_utterances(episode):\n",
        "        yield self.utterance_to_tensor(utterance)\n",
        "\n",
        "  def bool_to_tensor(self, arg):\n",
        "    if arg:\n",
        "      return torch.tensor([1.], dtype=torch.float)\n",
        "    else:\n",
        "      return torch.tensor([0.], dtype=torch.float)\n",
        "\n",
        "  def easy_trainset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    utts = [utt for utt in episode]\n",
        "    # now balance\n",
        "    ironic_utts = [utt for utt in episode if utt[3]]\n",
        "    non_ironic_utts_all = [utt for utt in episode if not utt[3]]\n",
        "    non_ironic_balanced = []\n",
        "    i = 0\n",
        "    for x in non_ironic_utts_all:\n",
        "      if i % 6 == 0:\n",
        "        non_ironic_balanced.append(x)\n",
        "      i += 1\n",
        "    balanced_utts = ironic_utts + non_ironic_balanced\n",
        "    random.shuffle(balanced_utts)\n",
        "    print(balanced_utts)\n",
        "    return balanced_utts\n",
        "\n",
        "  def trainset(self):\n",
        "    # use all episodes except SBep13 and SBep19 (devset and testset) and only use d and c speakers\n",
        "    utts = []\n",
        "    for name, episode in self.data.items():\n",
        "      if name == \"SBep19\" or name == \"SBep13\":\n",
        "        continue # dont include testset episode in trainset\n",
        "      for utterance in episode:\n",
        "        if utterance[2] in ['c', 'd']:\n",
        "          utts.append(utterance)\n",
        "    # shuffle and rebalance ironic and non ironic utts\n",
        "    random.shuffle(utts)\n",
        "    ironic = [utt for utt in utts if utt[3]]\n",
        "    non_ironic = [utt for utt in utts if not utt[3]]\n",
        "    max_len = min(len(ironic), len(non_ironic))\n",
        "    utts_balanced = ironic[0:max_len] + non_ironic[0:max_len]\n",
        "    random.shuffle(utts_balanced)\n",
        "    return utts_balanced\n",
        "\n",
        "  def testset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "  def devset(self):\n",
        "    episode = self.data[\"SBep19\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "corpus = PrunedCorpus('Irony-Recognition/AudioData/GatedPruned3')\n",
        "\n",
        "# look at distribution of speakers over episodes\n",
        "for episode in corpus.get_episodes():\n",
        "  speakers = set()\n",
        "  for utterance in corpus.get_utterances(episode):\n",
        "    speakers.add(utterance[2])\n",
        "  print(episode, speakers)\n",
        "\n",
        "# look at distribution of tags over episodes\n",
        "sarcastic_utts = []\n",
        "non_sarcastic_utts = []\n",
        "\n",
        "class SarcasmClassifier(torch.nn.Module):\n",
        "  # with LSTM pooling\n",
        "  def __init__(self, dim):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.lstm = torch.nn.LSTM(dim, 512, 1, batch_first=True, bidirectional=False)\n",
        "    self.mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(512, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "  def forward(self, x):\n",
        "    h, (hn, _) = self.lstm(x)\n",
        "    hn = hn.squeeze(0)\n",
        "    #print(hn.shape)\n",
        "    return self.mlp(hn)\n",
        "\n",
        "dim = 1024\n",
        "model = SarcasmClassifier(dim).to('cuda')\n",
        "\n",
        "# train\n",
        "trainset = corpus.trainset()\n",
        "# first look at balance of labels\n",
        "trainset_size = len(trainset)\n",
        "trainset_ironic_size = len([x for x in trainset if x[3]])\n",
        "trainset_non_ironic_size = trainset_size - trainset_ironic_size\n",
        "print(\"trainset data:\", trainset_size, trainset_ironic_size, trainset_non_ironic_size)\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRAIN_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    print(\"Epoch: \", epoch+1)\n",
        "    point_counter = 0\n",
        "    remaining_examples = trainset_size\n",
        "    i = 0\n",
        "    while remaining_examples >= BATCH_SIZE:\n",
        "      optimizer.zero_grad()\n",
        "      # get batch\n",
        "      batch_examples = trainset[i:i+BATCH_SIZE]\n",
        "      batch_labels = torch.stack([corpus.bool_to_tensor(x[3]) for x in batch_examples]).to('cuda')\n",
        "      batch_examples = [corpus.utterance_to_tensor(x) for x in batch_examples]\n",
        "      lengths = [seq.size(0) for seq in batch_examples]\n",
        "      padded_sequences = pad_sequence(batch_examples, batch_first=True)\n",
        "      packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
        "      i += BATCH_SIZE\n",
        "      remaining_examples -= BATCH_SIZE\n",
        "      # now calc loss and backprop\n",
        "      y_hat = model(packed_sequences)\n",
        "      #print(y_hat, batch_labels)\n",
        "      loss = criterion(y_hat, batch_labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      point_counter += 1\n",
        "      if point_counter % 10 == 0:\n",
        "          print(f\"Loss: {loss}, {point_counter*BATCH_SIZE}/{trainset_size}, {y_hat}, {batch_labels}\")\n",
        "\n",
        "    # evaluate\n",
        "    print(\"evaluating\")\n",
        "    with torch.no_grad():\n",
        "      evalset = corpus.devset()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for example in evalset:\n",
        "        x = corpus.utterance_to_tensor(example, with_grad=False)\n",
        "        y = corpus.bool_to_tensor(example[3])\n",
        "        y_hat = torch.sigmoid(model(x))\n",
        "        y_pred = torch.round(y_hat)\n",
        "        print(x.shape, y, y_pred, y_hat)\n",
        "        if (y_hat[0] >= 0.5 and y[0] >= 0.5) or (y_hat[0] < 0.5 and y[0] < 0.5):\n",
        "          correct += 1\n",
        "        total += 1\n",
        "      print(f'{correct}/{total}')"
      ],
      "metadata": {
        "id": "qBloo-iP7ry6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRP642kMzsje"
      },
      "source": [
        "# Pre-training Wav2Vec on Emotion Detection then Fine-Tuning for Irony Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To augment our small dataset, I will see if improvement can be made by fine tuning first on a related dataset, and then further fine tuning on our SadBoyz dataset. The related dataset I have selected is the ravdess emotion classification dataset. The most attractive property of this dataset is that it features the same sentence said different times with different emotional prosody, which should help us detect irony using only prosodic cues."
      ],
      "metadata": {
        "id": "mipzIJog0Q6t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTmwlzAp0Pjn"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "import os, re, random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from transformers import Wav2Vec2Model, Wav2Vec2Tokenizer\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "from scipy.signal import decimate\n",
        "\n",
        "class EmotionDataset:\n",
        "\n",
        "  def __init__(self, wav2vec_tokenizer, wav2vec_model):\n",
        "    dataset = load_dataset(\"narad/ravdess\", split=\"train\")\n",
        "    dataset = dataset.train_test_split(test_size=0.1)\n",
        "    self.train_dataset = dataset[\"train\"]\n",
        "    self.test_dataset = dataset[\"test\"]\n",
        "    print(self.train_dataset, self.test_dataset)\n",
        "    self.tokenizer = wav2vec_tokenizer\n",
        "    self.model_wav2vec = wav2vec_model\n",
        "\n",
        "  def trainset(self, with_grad=False):\n",
        "    self.train_dataset.shuffle()\n",
        "    for example in self.train_dataset:\n",
        "      audio_input, sample_rate = sf.read(example['audio']['path'])\n",
        "      #print(sample_rate, type(audio_input), audio_input.shape, audio_input)\n",
        "      # decimate by factor of 3 to downsample 48kHZ to 16kHZ\n",
        "      try:\n",
        "        resampled_data = decimate(audio_input, 3)\n",
        "      except:\n",
        "        print(\"!!!\")\n",
        "        yield None, None\n",
        "      input_values = self.tokenizer(resampled_data, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "      input_values = input_values.to('cuda')\n",
        "      if with_grad:\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      tokens = h.squeeze(0)\n",
        "      #print(\"example['labels']\", example['labels'])\n",
        "      label = torch.tensor(example['labels'], dtype=torch.long)\n",
        "      yield tokens, label\n",
        "\n",
        "  def testset(self):\n",
        "    print(self.test_dataset)\n",
        "    for example in self.test_dataset:\n",
        "      audio_input, sample_rate = sf.read(example['audio']['path'])\n",
        "      #print(sample_rate, type(audio_input), audio_input.shape, audio_input)\n",
        "      # decimate by factor of 3 to downsample 48kHZ to 16kHZ\n",
        "      try:\n",
        "        resampled_data = decimate(audio_input, 3)\n",
        "      except:\n",
        "        print(\"!!!\")\n",
        "        yield None, None\n",
        "      input_values = self.tokenizer(resampled_data, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "      input_values = input_values.to('cuda')\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      tokens = h.squeeze(0)\n",
        "      label = torch.zeros(8, dtype=torch.float)\n",
        "      #print(example)\n",
        "      print(\"example['labels']\", example['labels'])\n",
        "      label[example['labels']] = 1.0\n",
        "      yield tokens, label\n",
        "\n",
        "class IronyCorpus:\n",
        "\n",
        "  def __init__(self, path, tokenizer, model):\n",
        "    self.len = 0\n",
        "    self.path = path\n",
        "    self.data = {} # key is episode num, val is list of (utterance file, utterance num, speaker, is_sarcasm) tuple\n",
        "    self.load_data()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model_wav2vec = model\n",
        "\n",
        "  def load_data(self):\n",
        "    for filename in os.listdir(self.path):\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "      self.len += 1\n",
        "      filename_split = filename.split('_')\n",
        "      episode_label = filename_split[0]\n",
        "      if episode_label not in self.data:\n",
        "        self.data[episode_label] = []\n",
        "      utterance_num = re.findall(r'\\d+', filename_split[1])[0]\n",
        "      speaker = filename_split[1][0]\n",
        "      is_sarcasm = \"-I.wav\" in filename\n",
        "      #print(filename, utterance_num, speaker, is_sarcasm)\n",
        "      self.data[episode_label].append((filename, utterance_num, speaker, is_sarcasm))\n",
        "    # sort the utterances once all files have been read\n",
        "    for episode, utterances in self.data.items():\n",
        "      episode_sorted = sorted(utterances, key=lambda x: int(x[1]))\n",
        "      self.data[episode] = episode_sorted\n",
        "\n",
        "  def get_episodes(self):\n",
        "    return list(self.data.keys())\n",
        "\n",
        "  def get_utterances(self, episode):\n",
        "    return self.data[episode]\n",
        "\n",
        "  def utterance_to_tensor(self, utterance:tuple, with_grad=False):\n",
        "    audio_input, _ = sf.read(self.path + \"/\" + utterance[0])\n",
        "    input_values = self.tokenizer(audio_input, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "    input_values = input_values.to('cuda')\n",
        "    if with_grad:\n",
        "      h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    return h.squeeze(0)\n",
        "\n",
        "  def __iter__(self):\n",
        "    for episode in self.get_episodes():\n",
        "      for utterance in self.get_utterances(episode):\n",
        "        yield self.utterance_to_tensor(utterance)\n",
        "\n",
        "  def bool_to_tensor(self, arg):\n",
        "    if arg:\n",
        "      return torch.tensor([1.], dtype=torch.float)\n",
        "    else:\n",
        "      return torch.tensor([0.], dtype=torch.float)\n",
        "\n",
        "  def easy_trainset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    utts = [utt for utt in episode]\n",
        "    # now balance\n",
        "    ironic_utts = [utt for utt in episode if utt[3]]\n",
        "    non_ironic_utts_all = [utt for utt in episode if not utt[3]]\n",
        "    non_ironic_balanced = []\n",
        "    i = 0\n",
        "    for x in non_ironic_utts_all:\n",
        "      if i % 6 == 0:\n",
        "        non_ironic_balanced.append(x)\n",
        "      i += 1\n",
        "    balanced_utts = ironic_utts + non_ironic_balanced\n",
        "    random.shuffle(balanced_utts)\n",
        "    print(balanced_utts)\n",
        "    return balanced_utts\n",
        "\n",
        "  def trainset(self, shuffle=True):\n",
        "    # use all episodes except SBep13 and SBep19 (devset and testset) and only use d and c speakers\n",
        "    utts = []\n",
        "    for name, episode in self.data.items():\n",
        "      if name == \"SBep19\" or name == \"SBep13\":\n",
        "        continue # dont include testset episode in trainset\n",
        "      for utterance in episode:\n",
        "        if utterance[2] in ['c', 'd']:\n",
        "          utts.append(utterance)\n",
        "    # shuffle and rebalance ironic and non ironic utts\n",
        "    random.shuffle(utts)\n",
        "    ironic = [utt for utt in utts if utt[3]]\n",
        "    non_ironic = [utt for utt in utts if not utt[3]]\n",
        "    max_len = min(len(ironic), len(non_ironic))\n",
        "    utts_balanced = ironic[0:max_len] + non_ironic[0:max_len]\n",
        "    random.shuffle(utts_balanced)\n",
        "    return utts_balanced\n",
        "\n",
        "  def devset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "  def testset(self):\n",
        "    episode = self.data[\"SBep19\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "class SarcasmClassifier(torch.nn.Module):\n",
        "  # with LSTM pooling\n",
        "  def __init__(self, dim):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.pretrain = True # use a different output transformation for emotion dataset (8 classes) and irony dataset (2) classes\n",
        "    self.lstm = torch.nn.LSTM(dim, 512, 1, batch_first=True, bidirectional=False)\n",
        "    self.out_transform_pretrain = torch.nn.Linear(512, 8)\n",
        "    self.out_transform_finetune = torch.nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #try:\n",
        "    #  print(x.shape, x)\n",
        "    #except:\n",
        "    #  print(x.data.shape, x.data)\n",
        "    h, (hn, _) = self.lstm(x)\n",
        "    hn = hn.squeeze(0)\n",
        "    #print(hn.shape)\n",
        "    if self.pretrain:\n",
        "      return self.out_transform_pretrain(hn)\n",
        "    else:\n",
        "      return self.out_transform_finetune(hn)\n",
        "\n",
        "dim = 768\n",
        "model = SarcasmClassifier(dim).to('cuda')\n",
        "\n",
        "wav2vec_model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\").to('cuda') # fine tune this model on both datasets\n",
        "wav2vec_tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# first fine tune on emtion dataset\n",
        "emotion_dataset = EmotionDataset(wav2vec_tokenizer, wav2vec_model)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRAIN_EPOCHS = 5\n",
        "point_counter = 0\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "  print(\"Emotion Epoch: \", epoch+1)\n",
        "  trainset = emotion_dataset.trainset()\n",
        "  for x, y in trainset:\n",
        "    try:\n",
        "      x = x.to('cuda')\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "      #print(x.shape, y, y_hat)\n",
        "      loss = criterion(y_hat, y)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      point_counter += 1\n",
        "      if point_counter % 500 == 0:\n",
        "          print(f\"Loss: {loss}, {point_counter}, {y}, {y_hat}\")\n",
        "    except Exception as e:\n",
        "      print(\"Err\", e)\n",
        "\n",
        "  # evaluate\n",
        "  print(\"evaluating\")\n",
        "  with torch.no_grad():\n",
        "    evalset = emotion_dataset.testset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in evalset:\n",
        "      try:\n",
        "        #print(x.shape,y.shape)\n",
        "        y_hat = model(x)\n",
        "        y_pred = torch.argmax(y_hat, dim=0)\n",
        "        print(y_hat, y_pred, y, torch.argmax(y, dim=0))\n",
        "        if y_pred == torch.argmax(y, dim=0):\n",
        "          correct += 1\n",
        "        total += 1\n",
        "      except Exception as e:\n",
        "        print(\"Err:\", e)\n",
        "    print(f'{correct}/{total}')\n",
        "\n",
        "# then fine tune on irony dataset\n",
        "model.pretrain = False\n",
        "ironony_dataset = IronyCorpus('Irony-Recognition/AudioData/GatedPruned3', wav2vec_tokenizer, wav2vec_model)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "TRAIN_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    print(\"Irony Epoch: \", epoch+1)\n",
        "    trainset = ironony_dataset.trainset(shuffle=True)\n",
        "    trainset_size = len(trainset)\n",
        "    remaining_examples = trainset_size\n",
        "    point_counter = 0\n",
        "    i = 0\n",
        "    while remaining_examples >= BATCH_SIZE:\n",
        "      try:\n",
        "        optimizer.zero_grad()\n",
        "        # get batch\n",
        "        batch_examples = trainset[i:i+BATCH_SIZE]\n",
        "        batch_labels = torch.stack([ironony_dataset.bool_to_tensor(x[3]) for x in batch_examples])\n",
        "        batch_examples = [ironony_dataset.utterance_to_tensor(x) for x in batch_examples]\n",
        "        lengths = [seq.size(0) for seq in batch_examples]\n",
        "        padded_sequences = pad_sequence(batch_examples, batch_first=True)\n",
        "        packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
        "        i += BATCH_SIZE\n",
        "        remaining_examples -= BATCH_SIZE\n",
        "        # now calc loss and backprop\n",
        "        y_hat = model(packed_sequences)\n",
        "        loss = criterion(y_hat, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        point_counter += 1\n",
        "        if point_counter % 32 == 0:\n",
        "            print(f\"Loss: {loss}, {point_counter*BATCH_SIZE}/{trainset_size}, {y_hat}, {batch_labels}\")\n",
        "      except Exception as e:\n",
        "        print(\"Err\", e)\n",
        "    # evaluate\n",
        "    print(\"evaluating\")\n",
        "    with torch.no_grad():\n",
        "      evalset = ironony_dataset.testset()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for example in evalset:\n",
        "        try:\n",
        "          x = ironony_dataset.utterance_to_tensor(example, with_grad=False)\n",
        "          y = ironony_dataset.bool_to_tensor(example[3])\n",
        "          y_hat = torch.sigmoid(model(x))\n",
        "          y_pred = torch.round(y_hat)\n",
        "          print(x.shape, y, y_pred, y_hat)\n",
        "          if (y_hat[0] >= 0.5 and y[0] >= 0.5) or (y_hat[0] < 0.5 and y[0] < 0.5):\n",
        "            correct += 1\n",
        "          total += 1\n",
        "        except Exception as e:\n",
        "          print(\"Err:\", e)\n",
        "      print(f'{correct}/{total}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, I do the same thing but I instead use Wav2Vec-Conformer"
      ],
      "metadata": {
        "id": "tJ-Y1BazxNY-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zD6UFMeAgOwp"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "\n",
        "import os, re, random\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
        "from transformers import Wav2Vec2ConformerModel, AutoProcessor\n",
        "import soundfile as sf\n",
        "from datasets import load_dataset\n",
        "from scipy.signal import decimate\n",
        "\n",
        "class EmotionDataset:\n",
        "\n",
        "  def __init__(self, wav2vec_tokenizer, wav2vec_model):\n",
        "    dataset = load_dataset(\"narad/ravdess\", split=\"train\")\n",
        "    dataset = dataset.train_test_split(test_size=0.1)\n",
        "    self.train_dataset = dataset[\"train\"]\n",
        "    self.test_dataset = dataset[\"test\"]\n",
        "    print(self.train_dataset, self.test_dataset)\n",
        "    self.tokenizer = wav2vec_tokenizer\n",
        "    self.model_wav2vec = wav2vec_model\n",
        "\n",
        "  def trainset(self, with_grad=False):\n",
        "    self.train_dataset.shuffle()\n",
        "    for example in self.train_dataset:\n",
        "      audio_input, sample_rate = sf.read(example['audio']['path'])\n",
        "      #print(sample_rate, type(audio_input), audio_input.shape, audio_input)\n",
        "      # decimate by factor of 3 to downsample 48kHZ to 16kHZ\n",
        "      try:\n",
        "        resampled_data = decimate(audio_input, 3)\n",
        "      except:\n",
        "        print(\"!!!\")\n",
        "        yield None, None\n",
        "      input_values = self.tokenizer(resampled_data, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "      input_values = input_values.to('cuda')\n",
        "      if with_grad:\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      else:\n",
        "        with torch.no_grad():\n",
        "          h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      tokens = h.squeeze(0)\n",
        "      #print(\"example['labels']\", example['labels'])\n",
        "      label = torch.tensor(example['labels'], dtype=torch.long)\n",
        "      yield tokens, label\n",
        "\n",
        "  def testset(self):\n",
        "    print(self.test_dataset)\n",
        "    for example in self.test_dataset:\n",
        "      audio_input, sample_rate = sf.read(example['audio']['path'])\n",
        "      #print(sample_rate, type(audio_input), audio_input.shape, audio_input)\n",
        "      # decimate by factor of 3 to downsample 48kHZ to 16kHZ\n",
        "      try:\n",
        "        resampled_data = decimate(audio_input, 3)\n",
        "      except:\n",
        "        print(\"!!!\")\n",
        "        yield None, None\n",
        "      input_values = self.tokenizer(resampled_data, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "      input_values = input_values.to('cuda')\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "      tokens = h.squeeze(0)\n",
        "      label = torch.zeros(8, dtype=torch.float).to('cuda')\n",
        "      #print(example)\n",
        "      print(\"example['labels']\", example['labels'])\n",
        "      label[example['labels']] = 1.0\n",
        "      yield tokens, label\n",
        "\n",
        "class IronyCorpus:\n",
        "\n",
        "  def __init__(self, path, tokenizer, model):\n",
        "    self.len = 0\n",
        "    self.path = path\n",
        "    self.data = {} # key is episode num, val is list of (utterance file, utterance num, speaker, is_sarcasm) tuple\n",
        "    self.load_data()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.model_wav2vec = model\n",
        "\n",
        "  def load_data(self):\n",
        "    for filename in os.listdir(self.path):\n",
        "      if filename == '.DS_Store':\n",
        "        continue\n",
        "      self.len += 1\n",
        "      filename_split = filename.split('_')\n",
        "      episode_label = filename_split[0]\n",
        "      if episode_label not in self.data:\n",
        "        self.data[episode_label] = []\n",
        "      utterance_num = re.findall(r'\\d+', filename_split[1])[0]\n",
        "      speaker = filename_split[1][0]\n",
        "      is_sarcasm = \"-I.wav\" in filename\n",
        "      #print(filename, utterance_num, speaker, is_sarcasm)\n",
        "      self.data[episode_label].append((filename, utterance_num, speaker, is_sarcasm))\n",
        "    # sort the utterances once all files have been read\n",
        "    for episode, utterances in self.data.items():\n",
        "      episode_sorted = sorted(utterances, key=lambda x: int(x[1]))\n",
        "      self.data[episode] = episode_sorted\n",
        "\n",
        "  def get_episodes(self):\n",
        "    return list(self.data.keys())\n",
        "\n",
        "  def get_utterances(self, episode):\n",
        "    return self.data[episode]\n",
        "\n",
        "  def utterance_to_tensor(self, utterance:tuple, with_grad=False):\n",
        "    audio_input, _ = sf.read(self.path + \"/\" + utterance[0])\n",
        "    input_values = self.tokenizer(audio_input, return_tensors=\"pt\", padding=\"longest\", sampling_rate=16000).input_values\n",
        "    input_values = input_values.to('cuda')\n",
        "    if with_grad:\n",
        "      h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    else:\n",
        "      with torch.no_grad():\n",
        "        h = self.model_wav2vec(input_values).last_hidden_state\n",
        "    return h.squeeze(0)\n",
        "\n",
        "  def __iter__(self):\n",
        "    for episode in self.get_episodes():\n",
        "      for utterance in self.get_utterances(episode):\n",
        "        yield self.utterance_to_tensor(utterance)\n",
        "\n",
        "  def bool_to_tensor(self, arg):\n",
        "    if arg:\n",
        "      return torch.tensor([1.], dtype=torch.float).to('cuda')\n",
        "    else:\n",
        "      return torch.tensor([0.], dtype=torch.float).to('cuda')\n",
        "\n",
        "  def easy_trainset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    utts = [utt for utt in episode]\n",
        "    # now balance\n",
        "    ironic_utts = [utt for utt in episode if utt[3]]\n",
        "    non_ironic_utts_all = [utt for utt in episode if not utt[3]]\n",
        "    non_ironic_balanced = []\n",
        "    i = 0\n",
        "    for x in non_ironic_utts_all:\n",
        "      if i % 6 == 0:\n",
        "        non_ironic_balanced.append(x)\n",
        "      i += 1\n",
        "    balanced_utts = ironic_utts + non_ironic_balanced\n",
        "    random.shuffle(balanced_utts)\n",
        "    print(balanced_utts)\n",
        "    return balanced_utts\n",
        "\n",
        "  def trainset(self, shuffle=True):\n",
        "    # use all episodes except SBep13 and SBep19 (devset and testset) and only use d and c speakers\n",
        "    utts = []\n",
        "    for name, episode in self.data.items():\n",
        "      if name == \"SBep19\" or name == \"SBep13\":\n",
        "        continue # dont include testset episode in trainset\n",
        "      for utterance in episode:\n",
        "        if utterance[2] in ['c', 'd']:\n",
        "          utts.append(utterance)\n",
        "    # shuffle and rebalance ironic and non ironic utts\n",
        "    random.shuffle(utts)\n",
        "    ironic = [utt for utt in utts if utt[3]]\n",
        "    non_ironic = [utt for utt in utts if not utt[3]]\n",
        "    max_len = min(len(ironic), len(non_ironic))\n",
        "    utts_balanced = ironic[0:max_len] + non_ironic[0:max_len]\n",
        "    random.shuffle(utts_balanced)\n",
        "    return utts_balanced\n",
        "\n",
        "  def devset(self):\n",
        "    episode = self.data[\"SBep13\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "  def testset(self):\n",
        "    episode = self.data[\"SBep19\"]\n",
        "    return [utt for utt in episode]\n",
        "\n",
        "class SarcasmClassifier(torch.nn.Module):\n",
        "  # with LSTM pooling\n",
        "  def __init__(self, dim):\n",
        "    super(SarcasmClassifier, self).__init__()\n",
        "    self.pretrain = True # use a different output transformation for emotion dataset (8 classes) and irony dataset (2) classes\n",
        "    self.lstm = torch.nn.LSTM(dim, 512, 1, batch_first=True, bidirectional=False)\n",
        "    self.out_transform_pretrain = torch.nn.Linear(512, 8)\n",
        "    self.out_transform_finetune = torch.nn.Linear(512, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #try:\n",
        "    #  print(x.shape, x)\n",
        "    #except:\n",
        "    #  print(x.data.shape, x.data)\n",
        "    h, (hn, _) = self.lstm(x)\n",
        "    hn = hn.squeeze(0)\n",
        "    #print(hn.shape)\n",
        "    if self.pretrain:\n",
        "      return self.out_transform_pretrain(hn)\n",
        "    else:\n",
        "      return self.out_transform_finetune(hn)\n",
        "\n",
        "dim = 1024\n",
        "model = SarcasmClassifier(dim).to('cuda')\n",
        "\n",
        "wav2vec_model = Wav2Vec2ConformerModel.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\").to('cuda') # fine tune this model on both datasets\n",
        "wav2vec_tokenizer = AutoProcessor.from_pretrained(\"facebook/wav2vec2-conformer-rope-large-960h-ft\")\n",
        "\n",
        "# first fine tune on emtion dataset\n",
        "emotion_dataset = EmotionDataset(wav2vec_tokenizer, wav2vec_model)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "TRAIN_EPOCHS = 5\n",
        "point_counter = 0\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "  print(\"Emotion Epoch: \", epoch+1)\n",
        "  trainset = emotion_dataset.trainset()\n",
        "  for x, y in trainset:\n",
        "    try:\n",
        "      x = x.to('cuda')\n",
        "      optimizer.zero_grad()\n",
        "      y_hat = model(x)\n",
        "      #print(x.shape, y, y_hat)\n",
        "      loss = criterion(y_hat, y.to('cuda'))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      point_counter += 1\n",
        "      if point_counter % 500 == 0:\n",
        "          print(f\"Loss: {loss}, {point_counter}, {y}, {y_hat}\")\n",
        "    except Exception as e:\n",
        "      print(\"Err\", e)\n",
        "\n",
        "  # evaluate\n",
        "  print(\"evaluating\")\n",
        "  with torch.no_grad():\n",
        "    evalset = emotion_dataset.testset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x, y in evalset:\n",
        "      try:\n",
        "        #print(x.shape,y.shape)\n",
        "        y_hat = model(x)\n",
        "        y_pred = torch.argmax(y_hat, dim=0)\n",
        "        print(y_hat, y_pred, y, torch.argmax(y, dim=0))\n",
        "        if y_pred == torch.argmax(y, dim=0):\n",
        "          correct += 1\n",
        "        total += 1\n",
        "      except Exception as e:\n",
        "        print(\"Err:\", e)\n",
        "    print(f'{correct}/{total}')\n",
        "\n",
        "# then fine tune on irony dataset\n",
        "model.pretrain = False\n",
        "ironony_dataset = IronyCorpus('Irony-Recognition/AudioData/GatedPruned3', wav2vec_tokenizer, wav2vec_model)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "TRAIN_EPOCHS = 10\n",
        "BATCH_SIZE = 16\n",
        "for epoch in range(TRAIN_EPOCHS):\n",
        "    print(\"Irony Epoch: \", epoch+1)\n",
        "    trainset = ironony_dataset.trainset(shuffle=True)\n",
        "    trainset_size = len(trainset)\n",
        "    remaining_examples = trainset_size\n",
        "    point_counter = 0\n",
        "    i = 0\n",
        "    while remaining_examples >= BATCH_SIZE:\n",
        "      try:\n",
        "        optimizer.zero_grad()\n",
        "        # get batch\n",
        "        batch_examples = trainset[i:i+BATCH_SIZE]\n",
        "        batch_labels = torch.stack([ironony_dataset.bool_to_tensor(x[3]) for x in batch_examples]).to('cuda')\n",
        "        batch_examples = [ironony_dataset.utterance_to_tensor(x) for x in batch_examples]\n",
        "        lengths = [seq.size(0) for seq in batch_examples]\n",
        "        padded_sequences = pad_sequence(batch_examples, batch_first=True)\n",
        "        packed_sequences = pack_padded_sequence(padded_sequences, lengths, batch_first=True, enforce_sorted=False)\n",
        "        i += BATCH_SIZE\n",
        "        remaining_examples -= BATCH_SIZE\n",
        "        # now calc loss and backprop\n",
        "        y_hat = model(packed_sequences)\n",
        "        loss = criterion(y_hat, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        point_counter += 1\n",
        "        if point_counter % 32 == 0:\n",
        "            print(f\"Loss: {loss}, {point_counter*BATCH_SIZE}/{trainset_size}, {y_hat}, {batch_labels}\")\n",
        "      except Exception as e:\n",
        "        print(\"Err\", e)\n",
        "    # evaluate\n",
        "    print(\"evaluating\")\n",
        "    with torch.no_grad():\n",
        "      evalset = ironony_dataset.testset()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for example in evalset:\n",
        "        try:\n",
        "          x = ironony_dataset.utterance_to_tensor(example, with_grad=False)\n",
        "          y = ironony_dataset.bool_to_tensor(example[3])\n",
        "          y_hat = torch.sigmoid(model(x))\n",
        "          y_pred = torch.round(y_hat)\n",
        "          print(x.shape, y, y_pred, y_hat)\n",
        "          if (y_hat[0] >= 0.5 and y[0] >= 0.5) or (y_hat[0] < 0.5 and y[0] < 0.5):\n",
        "            correct += 1\n",
        "          total += 1\n",
        "        except Exception as e:\n",
        "          print(\"Err:\", e)\n",
        "      print(f'{correct}/{total}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}